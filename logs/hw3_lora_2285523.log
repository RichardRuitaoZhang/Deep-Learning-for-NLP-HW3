Starting LoRA fine-tuning on GPU...
Mon Dec  1 12:10:39 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.86.15              Driver Version: 570.86.15      CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA A100-PCIE-40GB          On  |   00000000:06:00.0 Off |                    0 |
| N/A   23C    P0             30W /  250W |       1MiB /  40960MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
Using device: cuda
Loading pretrained weights from pretrain/model_weights ...
Freezing base model parameters...
Injecting LoRA modules into attention projections (Q, K, V, Out)...
LoRA parameter summary:
  Total parameters:      144004177
  Trainable parameters:  589824
  Trainable ratio:       0.4096%
Epoch 01 | train_loss = 0.1157 | val_acc = 0.2280
Epoch 02 | train_loss = 0.0005 | val_acc = 0.2340
Epoch 03 | train_loss = 0.0043 | val_acc = 0.2260
Total training time: 4.94 minutes
LoRA fine-tuning completed.
